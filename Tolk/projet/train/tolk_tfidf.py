# -*- coding: utf-8 -*-
"""Tolk_tfidf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IVWoyk3KG6_akC7jYGuMtGYdCCd5uxha
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Tensorflow & Pathlib librairies
import tensorflow as tf 
import pathlib 
import pandas as pd
import numpy as np
import os
import io
import warnings
import joblib
warnings.filterwarnings('ignore')
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import matthews_corrcoef
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve
from sklearn.model_selection import cross_val_score
from sklearn.dummy import DummyClassifier

import unidecode
import spacy


import re
import ast

from spacy.lang.fr.stop_words import STOP_WORDS
nlp = spacy.load("fr_core_news_sm")
# Start with loading all necessary libraries

from os import path

import matplotlib.pyplot as plt
# % matplotlib inline


# Print the entire text
pd.set_option("max_colwidth", 200)
pd.options.display.max_rows = 999


train= pd.read_csv("train.csv")
test= pd.read_csv("test.csv")
validation = pd.read_csv("validation.csv")

train["dataset"] = "train"
test["dataset"] = "test"
validation["dataset"] = "validation"
df = pd.concat([train,test,validation], axis= 0,ignore_index = True)



df["questionType"] .value_counts()

df.info()

df = df.sample(df.shape[0])

df["questionType"] .value_counts()


# Import Spacy and french initialisation

nlp.Defaults.stop_words |= {"bonjour","bonsoir","bjr","bsr","b","c","d","e","f",'g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v',"w","x","y","z"}

df["logs"]=df["logs"].astype("str")
df['text_clean'] = df['logs'].str.replace('\d+', '')
df["text_clean"]= df["text_clean"].apply(lambda x: re.sub(r'http\S+', '', x)) #delete http adresses

import unidecode
for i in range(0,len(df["text_clean"])):
    df["text_clean"][i] = unidecode.unidecode(df["text_clean"][i])

df['text_clean'] = df['text_clean'].str.replace(r"[^A-Za-z0-9 ]+", " ")
#df["text_clean"] = df["text_clean"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch in [" "]))
df["text_clean"] = df["text_clean"].apply(lambda x: x.lower().strip())
STOP_WORDS = list(STOP_WORDS)
for elem in range(0,len(STOP_WORDS)):
    STOP_WORDS[elem] = unidecode.unidecode(STOP_WORDS[elem])
STOP_WORDS = set(STOP_WORDS)
df["logs_clean"] = df["text_clean"].apply(lambda x: " ".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)]))


# ordinal encode target variable
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df["questionType"])
df["questionType_encoded"] = y_encoded

# Regroupement des 6 classes les moins représentées
label_encoder_reduced = LabelEncoder()
yseries_encoded = pd.Series(data=y_encoded)
yseries_reduced_encoded = yseries_encoded.apply(lambda x: 15 if x in(13,5,14,3,0,15) else x)

mask = yseries_reduced_encoded == 15

y_reduced = df["questionType"].reset_index(drop=True)
y_reduced[mask] = "classe_regroupée"

df["questionType_reduced_encoded"] = label_encoder_reduced.fit_transform(y_reduced)

# Train Test Split
train = df[df["dataset"] == "train"]
test = df[df["dataset"] == "test"]
validation = df[df["dataset"] == "validation"]

# Tfidf transformer 
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF vector
vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)
X_train = vectorizer.fit_transform(train["logs_clean"])
X_test = vectorizer.transform(test["logs_clean"])

# X is a generator. We can transform that as an array
dense_train = X_train.toarray()
dense_test = X_test.toarray()

# pd.DataFrame(dense, 
#              columns=[x for x in vectorizer.get_feature_names()], 
#              index=["doc_{}".format(x) for x in range(1, 8)] )

target = "questionType_encoded"

#target = "questionType_reduced_encoded"


y_train = train[target]
y_test = test[target]
y_validation = validation[target]


classifier = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 1,class_weight = "balanced")
#classifier = SVC(probability=True)
classifier.fit(X_train, y_train)

#prédictions sur le dataset de train
prob_train = classifier.predict_proba(X_train)
predictions_train = np.argmax(prob_train,axis=1)

#prédictions sur le dataset de test
prob_test = classifier.predict_proba(X_test)
predictions_test = np.argmax(prob_test,axis=1)

# Comparaison avec le cas d'un modèle totalement aléatoire
dummy_clf = DummyClassifier(strategy="stratified")
dummy_clf.fit(X_train, y_train)

prob_rand = dummy_clf.predict_proba(X_test)
predictions_rand = np.argmax(prob_rand,axis=1)

mccf = matthews_corrcoef(y_test, predictions_rand)
print("MCCF random",mccf)

mccf_test = matthews_corrcoef(y_test, predictions_test)
mccf_train = matthews_corrcoef(y_train, predictions_train)

print("MCCF train",mccf_train)
print("MCCF test",mccf_test)

confusion_matrix = tf.math.confusion_matrix(y_test, predictions_test)

import seaborn as sns
sns.heatmap(confusion_matrix,annot=True)

predictions_texte = label_encoder.inverse_transform(predictions_test)

X_validation = vectorizer.transform(validation["logs_clean"])

prob_val = classifier.predict_proba(X_validation)
predictions_val = np.argmax(prob_val,axis=1)

mccf = matthews_corrcoef(y_validation, predictions_val)
print("MCCF validation",mccf)



joblib.dump(classifier, "models/classifier.joblib")

joblib.dump(vectorizer, 'models/vectorizer.joblib')

joblib.dump(label_encoder, 'models/label_encoder.joblib')

joblib.dump(label_encoder_reduced, 'models/label_encoder_reduced.joblib')